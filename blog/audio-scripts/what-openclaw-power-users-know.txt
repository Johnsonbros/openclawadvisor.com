There's OpenClaw as documented — the Getting Started guide, the basic examples, the demos. And then there's OpenClaw as actually used in production — the hard-won lessons, the non-obvious tricks, the stuff you only learn after months of real-world use.

After eighteen months running a four-agent OpenClaw system for our plumbing business, here are seven things power users know that beginners don't.

Number one: session management is your secret weapon.

What beginners do? Run everything in one long conversation session. What power users know? Strategic session management is the key to agent reliability.

The problem with long sessions is that context windows have limits. Even with two hundred thousand token models, long sessions suffer from context dilution where important instructions get buried, drift where agent behavior shifts over time, token cost where you're paying for thousands of irrelevant tokens, and error accumulation where small mistakes compound.

The power user approach is session scoping. Create task-specific sessions with clear boundaries. Instead of running everything in one session, scope sessions by task. Customer support gets a session with today's date. Research gets a project-specific session. Automation gets a workflow-specific session.

Benefits? Fresh context for each task. Easier debugging with isolated logs. Predictable behavior. Lower token costs.

We use three session patterns. Daily sessions where customer support gets a new session each day with the previous day's insights summarized and injected. Task-based sessions where each research project or automation workflow gets its own session. And ephemeral sessions where one-off questions use temporary sessions that automatically clean up after twenty-four hours.

Number two: tool permissions are more important than prompts.

What beginners focus on? Writing the perfect system prompt. What power users know? Tool permissions determine what's actually possible.

Here's the hierarchy of control. At the foundation is infrastructure — Docker, network, filesystem — what the environment allows. Next is tool permissions — which tools, what scope — what actions are possible. Then system prompts — instructions and guidelines — how the agent should behave. And finally user prompts — requests and questions — what the user wants.

Your prompts are suggestions. Your tool permissions are law.

Practical example: instead of prompting "never delete files," configure the write tool to be read-only so it can only create files, not delete or modify them. Instead of prompting "only search approved websites," whitelist specific domains in the browser tool config. Result? The agent can't violate constraints even if prompted to.

Number three: structured outputs beat natural language responses.

What beginners expect? Conversational responses from agents. What power users demand? Structured, parseable output.

The problem with natural language is that when agents respond conversationally, it's hard to parse programmatically. Formats are inconsistent. Extraction is error-prone. And you waste tokens on filler words.

Power user pattern: force JSON output. For agent-to-agent communication and automation workflows, use structured outputs. Instead of asking "what's the customer's phone number" and getting back a sentence you have to parse, ask for structured output with a schema and get back clean JSON with phone, email, and name fields.

We have standard JSON schemas for customer information extraction, appointment scheduling, research report formatting, and decision logging. This makes automation reliable and testable.

Number four: memory management beats prompt engineering.

What beginners do? Craft elaborate system prompts with all context stuffed in. What power users do? Build external memory systems.

The limit of in-context learning is that even with huge context windows, in-prompt knowledge has problems. It's expensive because tokens cost money. It's static because it doesn't update easily. It's unstructured so it's hard to query precisely. And it's overwhelming because agent attention dilutes.

Power user approach: build external knowledge bases. Use vector databases for semantic search to store past customer interactions and query them later. Use relational databases like PostgreSQL for structured data like appointment tracking. Use file-based knowledge for documentation organized by products, procedures, customers, and lessons learned.

This is the retrieval augmented generation or RAG pattern. User asks a question. Agent searches the knowledge base. Retrieves relevant context. Generates an answer using that retrieved context.

Benefits? Always up-to-date because you can add new knowledge anytime. Scalable because you have unlimited knowledge with constant token cost. And auditable because you know which sources informed each answer.

Number five: approval workflows are non-negotiable for production.

What beginners assume? Agents should be fully autonomous. What power users know? Human in the loop prevents disasters.

Not every decision needs the same level of oversight. There's a spectrum of autonomy. Low-risk actions like answering FAQs or logging data need no approval and auto-execute. Medium-risk actions like ordering routine supplies need notification only. High-risk actions like new customer bookings or spending over five hundred dollars require approval. Critical actions like system changes or legal matters require multi-person approval.

We use a simple approval queue system that sends a Telegram notification, waits for a response or timeout, and either proceeds or denies based on the human decision.

Benefits of approval workflows: a safety net against errors or adversarial inputs, an audit trail of decisions, gradual trust building where you start cautious and relax over time, and compliance which is required for many regulated industries.

Number six: monitoring and observability are day-one requirements.

What beginners do? Run agents and hope for the best. What power users do? Instrument everything.

We monitor three categories. Performance metrics like token usage per session, response latency, API error rates, and tool execution success rates. Behavioral metrics like actions taken by type, approval request frequency, error patterns, and context window utilization. And business metrics like tasks completed successfully, customer satisfaction, time and cost saved, and ROI tracking.

Our monitoring stack sends logs to Loki and Grafana, metrics to Prometheus and Grafana, and traces to Jaeger. We've configured alerts for token usage spikes over ten thousand tokens per hour, unusual tool execution with commands outside normal patterns, error rate increases over five percent, and approval backlog growing over ten pending items.

We also use a debug log pattern where every important agent action gets logged with full context including timestamp, agent name, session ID, action taken, input and output, tools used, tokens used, latency, and whether approval was required. When something goes wrong, we can reconstruct exactly what happened.

Number seven: cost optimization matters more than you think.

What beginners think? API costs are negligible. What power users discover? Token costs add up fast.

Our cost journey: Month one was forty-five dollars just playing around with low usage. Month two was two hundred thirty dollars going live with customers. Month three was eight hundred ninety dollars unoptimized with context window bloat. Month four dropped back down to one hundred eighty dollars after optimization.

What we changed: session scoping for fresh context with fewer wasted tokens. Structured outputs for less verbose responses. Caching frequent queries using a vector database for common questions. Cheaper models for simple tasks like using a smaller model for FAQs and a premium model for complex reasoning. And prompt compression to remove unnecessary verbosity.

Cost optimization tactics include model selection by task where simple tasks use cheaper models, complex reasoning uses premium models, and known answers use caching for zero cost. Context window management where you don't include entire conversation history every time, you summarize old context, and use retrieval instead of stuffing prompts. Batch processing where you group similar requests and amortize setup costs. And smart caching with time-to-live based invalidation which saves thirty to forty percent on our system.

We track cost per agent, per session type, per tool invocation, and per customer interaction. This lets us identify and fix expensive patterns quickly.

And here's the bonus meta-lesson. The biggest thing power users know is that OpenClaw is a platform, not a product. Beginners expect it to just work out of the box. Power users understand they're building a custom system. You choose your architecture. You design your workflows. You implement your security. You optimize for your use case. This is a feature, not a bug. The flexibility is what makes OpenClaw powerful.